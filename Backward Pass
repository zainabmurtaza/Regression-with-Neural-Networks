'''
Y is the observed output and Yhat is the predicted output (i.e., output of the network)

Mean Squared Error derivative wrt Yhat is (Yhat-Y)
'''
def dMSELoss(Y,Yhat):
    return (Yhat-Y) 


'''
The derivative of sigmoid(A) with respect to A is A*(1-A) 
where * is elementwise multiplication 
'''
def dsigmoid(Z):
    A=sigmoid(Z)
    return (A*(1-A))


def drelu(Z):
    """
np.where(condition, x, y) for each element of the array returns x if condition is true otherwise returns y.
In this case for each element Z drelu=1 if the element is greater than 0 otherwise drelu=0
"""
    drelu=np.where(Z>0, 1.0, 0.0) 
    return drelu 

#Backward Pass Function with MSE

def backward_pass(parameters, cache, X, Y, Yhat):
    n=X.shape[1]
    dZ2=dMSELoss(ùëå,Yhat)
    dW2=(1/n)*np.dot(dZ2,cache["A1"].T)
    db2=(1/n)*np.sum(dZ2, axis=1, keepdims=True)
    dA1=np.dot(parameters["W2"].T,dZ2)
    dZ1=dA1*drelu(cache["Z1"])
    dW1=(1/n)*np.dot(dZ1,X.T)
    db1=(1/n)*np.sum(dZ1, axis=1, keepdims=True)
    gradients={"dW1": dW1,
             "db1": db1,
             "dW2":dW2,
              "db2":db2
              }
    return gradients
